{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b429f99d",
      "metadata": {},
      "source": [
        "### Setup (Colab): install dependencies\n",
        "\n",
        "Run this cell first in **Google Colab** to install PyTorch, MONAI, and other requirements. Skip if running locally with a configured environment.\n",
        "\n",
        "**Note:** This notebook is self-contained and does not require the `carotid` folder (e.g. for Colab when only the notebook is uploaded)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1088590d",
      "metadata": {},
      "source": [
        "# StrokeLink – Carotid segmentation model (notebook)\n",
        "\n",
        "**StrokeLink** (AI-driven carotid ultrasound for stroke triage in Rwanda). This notebook implements the ML track: Momot (2022) data, CLAHE + DWT preprocessing, Swin-UNETR 2D, IMT measurement. Outputs feed the FastAPI backend and Flutter app for CHW screening in Bumbogo/Gasabo. Clinical high-risk threshold: **IMT ≥ 0.9 mm**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ca4c5633",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q torch monai opencv-python-headless PyWavelets numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f67562",
      "metadata": {},
      "source": [
        "## 1. Imports, constants, and in-notebook utils (IMT + data QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c7bba0e",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, Optional, List, Dict\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "# All imports and constants (run once)\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Optional, List, Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai.losses import DiceCELoss, DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_DIR = Path(\"data\")\n",
        "MODEL_DIR = Path(\"models\")\n",
        "FIGURES_DIR = Path(\"figures\")\n",
        "IMT_HIGH_RISK_MM = 0.9  # Clinical threshold for stroke risk triage (capstone)\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "FIGURES_DIR.mkdir(exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e255a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- In-notebook IMT and data QA (no carotid package) ---\n",
        "\n",
        "# ----- IMT (Intima-Media Thickness) -----\n",
        "def get_interfaces_from_mask(mask, lumen_label=1, wall_label=2):\n",
        "    \"\"\"Lumen-Intima and Media-Adventitia interfaces per column. mask: (H,W) 0=bg, 1=lumen, 2=wall.\"\"\"\n",
        "    h, w = mask.shape\n",
        "    lumen_intima = np.full(w, np.nan)\n",
        "    media_adventitia = np.full(w, np.nan)\n",
        "    for x in range(w):\n",
        "        col = mask[:, x]\n",
        "        lumen_idx = np.where(col == lumen_label)[0]\n",
        "        wall_idx = np.where(col == wall_label)[0]\n",
        "        if len(lumen_idx) and len(wall_idx):\n",
        "            lumen_center = np.mean(lumen_idx)\n",
        "            wall_inner = wall_idx[np.argmin(np.abs(wall_idx - lumen_center))]\n",
        "            lumen_intima[x] = float(wall_inner)\n",
        "            wall_outer = wall_idx[np.argmax(np.abs(wall_idx - lumen_center))]\n",
        "            media_adventitia[x] = float(wall_outer)\n",
        "        elif len(wall_idx) >= 2:\n",
        "            lumen_intima[x] = float(np.min(wall_idx))\n",
        "            media_adventitia[x] = float(np.max(wall_idx))\n",
        "    return lumen_intima, media_adventitia\n",
        "\n",
        "def imt_pixels_per_column(lumen_intima, media_adventitia):\n",
        "    \"\"\"Vertical distance (pixels) between inner and outer wall per column.\"\"\"\n",
        "    valid = np.isfinite(lumen_intima) & np.isfinite(media_adventitia)\n",
        "    thickness = np.abs(media_adventitia - lumen_intima)\n",
        "    thickness[~valid] = np.nan\n",
        "    return thickness\n",
        "\n",
        "def imt_mm_from_mask(mask, spacing_mm_per_pixel, lumen_label=1, wall_label=2):\n",
        "    \"\"\"Mean IMT in mm from segmentation mask.\"\"\"\n",
        "    li, ma = get_interfaces_from_mask(mask, lumen_label=lumen_label, wall_label=wall_label)\n",
        "    thickness_px = imt_pixels_per_column(li, ma)\n",
        "    valid = np.isfinite(thickness_px)\n",
        "    if not np.any(valid):\n",
        "        return np.nan\n",
        "    return float(np.nanmean(thickness_px) * spacing_mm_per_pixel)\n",
        "\n",
        "def imt_mae_mm(pred_masks, gt_masks, spacing_mm_per_pixel, lumen_label=1, wall_label=2):\n",
        "    \"\"\"Mean Absolute Error of IMT (mm) across batch. pred_masks, gt_masks: (N,H,W).\"\"\"\n",
        "    n = pred_masks.shape[0]\n",
        "    errors = []\n",
        "    for i in range(n):\n",
        "        pred_imt = imt_mm_from_mask(pred_masks[i], spacing_mm_per_pixel, lumen_label=lumen_label, wall_label=wall_label)\n",
        "        gt_imt = imt_mm_from_mask(gt_masks[i], spacing_mm_per_pixel, lumen_label=lumen_label, wall_label=wall_label)\n",
        "        if np.isfinite(pred_imt) and np.isfinite(gt_imt):\n",
        "            errors.append(abs(pred_imt - gt_imt))\n",
        "    return float(np.mean(errors)) if errors else np.nan\n",
        "\n",
        "# ----- Data QA -----\n",
        "def validate_image_readable(path):\n",
        "    \"\"\"Check if image loads. Returns (ok, error_msg).\"\"\"\n",
        "    try:\n",
        "        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            img = cv2.imread(str(path))\n",
        "            if img is None:\n",
        "                return False, \"Failed to load image\"\n",
        "            img = img[:, :, 0] if img.ndim == 3 else img\n",
        "        if img.size == 0 or img.ndim != 2:\n",
        "            return False, f\"Invalid shape: {getattr(img, 'shape', 'unknown')}\"\n",
        "        if np.all(img == img.flat[0]):\n",
        "            return False, \"Image is constant (possibly corrupted)\"\n",
        "        return True, None\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "def validate_mask_readable(path):\n",
        "    \"\"\"Check if mask loads. Returns (ok, error_msg).\"\"\"\n",
        "    try:\n",
        "        mask = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            mask = cv2.imread(str(path))\n",
        "            if mask is None:\n",
        "                return False, \"Failed to load mask\"\n",
        "            mask = mask[:, :, 0] if mask.ndim == 3 else mask\n",
        "        if mask.size == 0 or mask.ndim != 2:\n",
        "            return False, f\"Invalid shape: {getattr(mask, 'shape', 'unknown')}\"\n",
        "        return True, None\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "def check_mask_consistency(mask, img_shape=None, min_coverage_pct=0.001, max_coverage_pct=0.95):\n",
        "    \"\"\"Non-empty, reasonable coverage, optional shape match. Returns (ok, error_msg).\"\"\"\n",
        "    h, w = mask.shape\n",
        "    if img_shape is not None and (h, w) != img_shape:\n",
        "        return False, f\"Mask shape {mask.shape} != image shape {img_shape}\"\n",
        "    foreground = np.sum(mask > 127) if mask.dtype in (np.float32, np.float64) else np.sum(mask > 0)\n",
        "    total = h * w\n",
        "    coverage = foreground / total\n",
        "    if coverage < min_coverage_pct:\n",
        "        return False, f\"Mask nearly empty (coverage={coverage:.4f})\"\n",
        "    if coverage > max_coverage_pct:\n",
        "        return False, f\"Mask nearly full (coverage={coverage:.4f})\"\n",
        "    return True, None\n",
        "\n",
        "def validate_pair(img_path, mask_path, min_coverage_pct=0.001, max_coverage_pct=0.95, require_shape_match=True):\n",
        "    \"\"\"Validate image/mask pair. Returns (ok, error_msg).\"\"\"\n",
        "    ok, err = validate_image_readable(img_path)\n",
        "    if not ok:\n",
        "        return False, f\"Image: {err}\"\n",
        "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        img = cv2.imread(str(img_path))[:, :, 0]\n",
        "    img_shape = img.shape[:2]\n",
        "    ok, err = validate_mask_readable(mask_path)\n",
        "    if not ok:\n",
        "        return False, f\"Mask: {err}\"\n",
        "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        mask = cv2.imread(str(mask_path))[:, :, 0]\n",
        "    ok, err = check_mask_consistency(mask, img_shape=img_shape if require_shape_match else None,\n",
        "                                    min_coverage_pct=min_coverage_pct, max_coverage_pct=max_coverage_pct)\n",
        "    if not ok:\n",
        "        return False, err\n",
        "    return True, None\n",
        "\n",
        "def filter_and_flag_pairs(pairs, min_coverage_pct=0.001, max_coverage_pct=0.95):\n",
        "    \"\"\"Validate all pairs; return (valid_pairs, flagged_list).\"\"\"\n",
        "    valid, flagged = [], []\n",
        "    for img_path, mask_path in pairs:\n",
        "        ok, err = validate_pair(img_path, mask_path, min_coverage_pct=min_coverage_pct, max_coverage_pct=max_coverage_pct)\n",
        "        if ok:\n",
        "            valid.append((img_path, mask_path))\n",
        "        else:\n",
        "            flagged.append({\"img\": img_path, \"mask\": mask_path, \"reason\": err})\n",
        "    return valid, flagged\n",
        "\n",
        "print(\"IMT and data QA helpers defined (in-notebook).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3311e4fe",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing & Enhancement (definition)\n",
        "\n",
        "**Use this section.** It defines the single preprocessing pipeline. Section 3c only *applies* this when loading images.\n",
        "\n",
        "Since EDA showed mean intensities as low as ~13 (very dark), raw images are hard for the model to read. We apply:\n",
        "\n",
        "- **CLAHE (Contrast Limited Adaptive Histogram Equalization):** Redistributes pixel intensities so carotid wall boundaries are visible.\n",
        "- **DWT (Discrete Wavelet Transform) denoising:** Reduces ultrasound speckle while preserving sharp Intima–Media edges.\n",
        "- **Normalization:** Rescale pixel values to $[0, 1]$ so gradient descent converges faster.\n",
        "\n",
        "The code below defines the `cleaner` object; Section 3c uses `cleaner` when building the train/val/test arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ecc1751",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing: CLAHE + DWT; output in [0, 1]\n",
        "class MedicalDataCleaner:\n",
        "    \"\"\"CLAHE for contrast + DWT denoising for speckle (same as carotid/preprocessing.py).\"\"\"\n",
        "    def __init__(self, clahe_clip_limit=2.0, clahe_grid_size=(8, 8), dwt_wavelet=\"db4\", dwt_level=2, dwt_mode=\"soft\", dwt_threshold_scale=1.0):\n",
        "        self.clahe_clip_limit = clahe_clip_limit\n",
        "        self.clahe_grid_size = clahe_grid_size\n",
        "        self.dwt_wavelet = dwt_wavelet\n",
        "        self.dwt_level = dwt_level\n",
        "        self.dwt_mode = dwt_mode\n",
        "        self.dwt_threshold_scale = dwt_threshold_scale\n",
        "\n",
        "    def _clahe(self, img):\n",
        "        img = np.asarray(img, dtype=np.float64)\n",
        "        if img.max() > 1.0:\n",
        "            img = img / (img.max() + 1e-8)\n",
        "        img_uint8 = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clahe_clip_limit, tileGridSize=self.clahe_grid_size)\n",
        "        out = clahe.apply(img_uint8) if img_uint8.ndim == 2 else cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)\n",
        "        if img_uint8.ndim == 3:\n",
        "            out[..., 0] = clahe.apply(out[..., 0])\n",
        "            out = cv2.cvtColor(out, cv2.COLOR_LAB2RGB)\n",
        "        return out.astype(np.float64) / 255.0\n",
        "\n",
        "    def _dwt_denoise_2d(self, img):\n",
        "        coeffs = pywt.wavedec2(img, self.dwt_wavelet, level=self.dwt_level)\n",
        "        cA = coeffs[0]\n",
        "        detail_list = list(coeffs[1:])\n",
        "        sigma = np.median(np.abs(cA)) / 0.6745 if cA.size else 1.0\n",
        "        thresh = self.dwt_threshold_scale * sigma * np.sqrt(2 * np.log(cA.size + 1e-8))\n",
        "        detail_list = [tuple(pywt.threshold(d, thresh, mode=self.dwt_mode) if d is not None else None for d in level) for level in detail_list]\n",
        "        return pywt.waverec2([cA] + detail_list, self.dwt_wavelet)[: img.shape[0], : img.shape[1]]\n",
        "\n",
        "    def _dwt_denoise(self, img):\n",
        "        img = np.asarray(img, dtype=np.float64)\n",
        "        if img.ndim == 3:\n",
        "            return np.stack([self._dwt_denoise_2d(img[..., c]) for c in range(img.shape[-1])], axis=-1)\n",
        "        return self._dwt_denoise_2d(img)\n",
        "\n",
        "    def __call__(self, img, apply_clahe=True, apply_dwt=True):\n",
        "        out = np.asarray(img, dtype=np.float64)\n",
        "        if out.max() > 1.0:\n",
        "            out = out / (out.max() + 1e-8)\n",
        "        if apply_clahe:\n",
        "            out = self._clahe(out)\n",
        "        if apply_dwt:\n",
        "            out = self._dwt_denoise(out)\n",
        "        return np.clip(out, 0, 1).astype(np.float32)\n",
        "\n",
        "cleaner = MedicalDataCleaner(clahe_clip_limit=2.0, clahe_grid_size=(8, 8), dwt_wavelet=\"db4\", dwt_level=2)\n",
        "\n",
        "# Demo on a synthetic 2D \"ultrasound-like\" image\n",
        "np.random.seed(RANDOM_STATE)\n",
        "h, w = 128, 224\n",
        "fake_img = np.random.rand(h, w).astype(np.float32) * 0.5 + 0.25\n",
        "fake_img[40:60, :] += 0.3  # simulate wall\n",
        "cleaned = cleaner(fake_img, apply_clahe=True, apply_dwt=True)\n",
        "print(f\"Original shape: {fake_img.shape}, cleaned shape: {cleaned.shape}, range: [{cleaned.min():.3f}, {cleaned.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clahe-dwt-noise-analysis",
      "metadata": {},
      "source": [
        "#### Impact of CLAHE and DWT on image noise\n",
        "\n",
        "**CLAHE (Contrast Limited Adaptive Histogram Equalization):** While primarily designed for contrast enhancement, CLAHE can have a mixed impact on noise. By increasing the contrast, especially in previously dark or uniform regions, it can sometimes make existing noise more visible. However, its *contrast-limited* aspect helps prevent excessive amplification of noise in very homogeneous areas. Its main goal is not noise reduction, but rather making features more distinguishable.\n",
        "\n",
        "**DWT (Discrete Wavelet Transform) denoising:** This technique is specifically applied for noise reduction. Ultrasound images are notoriously plagued by *speckle noise*, a granular noise that degrades image quality. DWT denoising works by transforming the image into wavelet coefficients, where noise (typically high-frequency details) can be identified and suppressed by thresholding. By applying a soft or hard threshold to these detail coefficients, the algorithm reduces speckle while aiming to preserve important structural edges (e.g. the intima–media layer).\n",
        "\n",
        "**Overall impact on noise:** The combination of CLAHE and DWT typically yields images that are both clearer in contrast and smoother, with less noise. The DWT component actively suppresses characteristic speckle, leading to a cleaner image for analysis or model training. This noise reduction is important because excessive noise can obscure features and hurt segmentation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164ffcc1",
      "metadata": {},
      "source": [
        "## 3. Data: Momot (2022) – Common Carotid Artery Ultrasound\n",
        "\n",
        "**(Colab)** Mount Drive and unzip the dataset — run the cell below in Colab to load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963b1d94",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip -o \"/content/drive/MyDrive/Common Carotid Artery Ultrasound Images.zip\" -d \"/content/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7277102",
      "metadata": {},
      "source": [
        "### 3a. Find pairs, data cleaning, and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c2d40a",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_ROOT = Path(\"/content/data\") if Path(\"/content/data\").exists() else Path(\"data\")\n",
        "\n",
        "def find_image_mask_pairs(root, exts=(\".png\", \".jpg\", \".jpeg\")):\n",
        "    root = Path(root)\n",
        "    images = [p for p in root.rglob(\"*\") if p.suffix.lower() in exts and \"mask\" not in p.name.lower()]\n",
        "    pairs = []\n",
        "    for img_path in images:\n",
        "        for mask_dir in (\"Masks\", \"masks\", \"Labels\", \"labels\", \"Mask\", \"mask\"):\n",
        "            mask_path = root / mask_dir / img_path.name\n",
        "            if mask_path.exists():\n",
        "                pairs.append((str(img_path), str(mask_path)))\n",
        "                break\n",
        "        else:\n",
        "            stem, suf = img_path.stem, img_path.suffix\n",
        "            mask_path = img_path.parent / f\"{stem}_mask{suf}\"\n",
        "            if mask_path.exists():\n",
        "                pairs.append((str(img_path), str(mask_path)))\n",
        "    return pairs\n",
        "\n",
        "pairs = find_image_mask_pairs(DATA_ROOT)\n",
        "if not pairs:\n",
        "    raise FileNotFoundError(f\"No image/mask pairs under {DATA_ROOT}.\")\n",
        "print(f\"Found {len(pairs)} pairs\")\n",
        "\n",
        "valid_pairs, flagged = filter_and_flag_pairs(pairs, min_coverage_pct=0.001, max_coverage_pct=0.95)\n",
        "pairs = valid_pairs\n",
        "if flagged:\n",
        "    print(f\"Flagged {len(flagged)} (removed):\")\n",
        "    for f in flagged[:5]:\n",
        "        print(f\"  - {Path(f['img']).name}: {f['reason']}\")\n",
        "print(f\"Valid: {len(pairs)}\")\n",
        "\n",
        "# 70/15/15 split (Train / Val / Test)\n",
        "train_pairs, rest_pairs = train_test_split(pairs, test_size=0.30, random_state=RANDOM_STATE)\n",
        "val_pairs, test_pairs = train_test_split(rest_pairs, test_size=0.50, random_state=RANDOM_STATE)\n",
        "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad2ad73",
      "metadata": {},
      "source": [
        "### 3b. Phase One: Raw Discovery EDA\n",
        "\n",
        "**When:** Immediately after downloading the Momot (2022) dataset, before any model or preprocessing code.\n",
        "\n",
        "**Goal:** Understand what you are working with and catch \"garbage\" data.\n",
        "\n",
        "1. **Label leakage:** Ensure there is no text on the ultrasound images (e.g. patient names, hospital tags) that the model could use to \"cheat.\"\n",
        "2. **Outlier detection:** Flag images that are too bright, too dark, or have severe motion blur.\n",
        "3. **Class imbalance:** Check normal vs high-risk (IMT ≥ 0.9 mm). If severely imbalanced (e.g. 2000 normal vs 200 high-risk), use oversampling or reweighting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243e7fcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Phase One Raw Discovery: use all valid pairs (train+val+test) before preprocessing ---\n",
        "all_pairs = train_pairs + val_pairs + test_pairs\n",
        "n_total = len(all_pairs)\n",
        "spacing_eda = 0.04\n",
        "\n",
        "# Build per-image stats: brightness, blur (Laplacian variance), shape, coverage, IMT\n",
        "brightness, blur_score, shapes_list, coverages, imts_list = [], [], [], [], []\n",
        "for img_path, mask_path in all_pairs:\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        img = cv2.imread(img_path)[:, :, 0]\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        mask = cv2.imread(mask_path)[:, :, 0]\n",
        "    shapes_list.append(img.shape)\n",
        "    gray = np.asarray(img, dtype=np.float32) / (np.max(img) + 1e-8)\n",
        "    brightness.append(np.mean(gray))\n",
        "    blur_score.append(cv2.Laplacian(img, cv2.CV_64F).var())  # low = blurry\n",
        "    mask_bin = (mask > 127).astype(np.float32)\n",
        "    coverages.append(np.mean(mask_bin))\n",
        "    try:\n",
        "        imt = imt_mm_from_mask(mask_bin.astype(np.int32), spacing_eda, lumen_label=2, wall_label=1)\n",
        "        imts_list.append(imt if np.isfinite(imt) else np.nan)\n",
        "    except Exception:\n",
        "        imts_list.append(np.nan)\n",
        "\n",
        "brightness = np.array(brightness)\n",
        "blur_score = np.array(blur_score)\n",
        "imts_arr = np.array(imts_list)\n",
        "imts_valid = imts_arr[np.isfinite(imts_arr)]\n",
        "\n",
        "print(\"=== 1. Label leakage (manual check) ===\")\n",
        "print(\"Inspect samples below for text/labels (patient name, hospital, dates). Remove or mask such images.\")\n",
        "n_show = min(9, n_total)\n",
        "idx_show = np.random.RandomState(RANDOM_STATE).choice(n_total, n_show, replace=False)\n",
        "fig1, ax1 = plt.subplots(3, 3, figsize=(9, 9))\n",
        "for k, i in enumerate(idx_show):\n",
        "    img = cv2.imread(all_pairs[i][0], cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        img = cv2.imread(all_pairs[i][0])[:, :, 0]\n",
        "    ax1.flat[k].imshow(img, cmap='gray')\n",
        "    ax1.flat[k].set_title(Path(all_pairs[i][0]).name[:18])\n",
        "    ax1.flat[k].axis('off')\n",
        "plt.suptitle('Label leakage check: look for text on images'); plt.tight_layout()\n",
        "FIGURES_DIR.mkdir(exist_ok=True)\n",
        "plt.savefig(FIGURES_DIR / 'eda_label_leakage_check.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== 2. Outlier detection (brightness & motion blur) ===\")\n",
        "b_lo, b_hi = np.percentile(brightness, 2), np.percentile(brightness, 98)\n",
        "blur_lo = np.percentile(blur_score[blur_score > 0], 5) if np.any(blur_score > 0) else 0\n",
        "too_dark = np.where(brightness < b_lo)[0]\n",
        "too_bright = np.where(brightness > b_hi)[0]\n",
        "too_blurry = np.where(blur_score < blur_lo)[0] if blur_lo > 0 else np.array([])\n",
        "print(f\"Brightness: median={np.median(brightness):.3f}, range [{b_lo:.3f}, {b_hi:.3f}]\")\n",
        "print(f\"Blur (Laplacian var): median={np.median(blur_score):.1f}; low = blurry\")\n",
        "print(f\"Flagged: too dark {len(too_dark)}, too bright {len(too_bright)}, too blurry {len(too_blurry)}\")\n",
        "if len(too_dark) + len(too_bright) + len(too_blurry) > 0:\n",
        "    outlier_idx = np.unique(np.concatenate([too_dark[:3], too_bright[:3], too_blurry[:3]]))[:6]\n",
        "    fig2, ax2 = plt.subplots(2, 3, figsize=(9, 6))\n",
        "    for k, i in enumerate(outlier_idx):\n",
        "        if i >= n_total:\n",
        "            break\n",
        "        img = cv2.imread(all_pairs[i][0], cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            img = cv2.imread(all_pairs[i][0])[:, :, 0]\n",
        "        ax2.flat[k].imshow(img, cmap='gray')\n",
        "        ax2.flat[k].set_title(f\"b={brightness[i]:.2f} blur={blur_score[i]:.0f}\")\n",
        "        ax2.flat[k].axis('off')\n",
        "    plt.suptitle('Sample outliers (brightness / blur)'); plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / 'eda_outliers.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n=== 3. Class imbalance (normal vs high-risk by IMT) ===\")\n",
        "normal = np.sum(imts_valid < IMT_HIGH_RISK_MM)\n",
        "high_risk = np.sum(imts_valid >= IMT_HIGH_RISK_MM)\n",
        "print(f\"IMT (mm): min={np.nanmin(imts_arr):.3f}, max={np.nanmax(imts_arr):.3f}, mean={np.nanmean(imts_valid):.3f}\")\n",
        "print(f\"Normal (IMT < {IMT_HIGH_RISK_MM} mm): {normal}  |  High-risk (IMT >= {IMT_HIGH_RISK_MM} mm): {high_risk}\")\n",
        "ratio = max(normal, high_risk) / (min(normal, high_risk) + 1e-8)\n",
        "if ratio > 3:\n",
        "    print(f\">> Severe imbalance (ratio {ratio:.1f}:1). Consider oversampling or class weights.\")\n",
        "else:\n",
        "    print(\">> Balance is acceptable.\")\n",
        "\n",
        "print(\"\\n=== Summary: shapes & mask coverage ===\")\n",
        "print(f\"Shapes: {set(shapes_list)}\")\n",
        "print(f\"Mask coverage: min={min(coverages):.4f}, max={max(coverages):.4f}, mean={np.mean(coverages):.4f}\")\n",
        "fig3, ax3 = plt.subplots(2, 3, figsize=(9, 6))\n",
        "sample_idx = np.random.RandomState(RANDOM_STATE).choice(n_total, min(6, n_total), replace=False)\n",
        "for k, i in enumerate(sample_idx):\n",
        "    img = cv2.imread(all_pairs[i][0], cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.imread(all_pairs[i][1], cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        img = cv2.imread(all_pairs[i][0])[:, :, 0]\n",
        "    if mask is None:\n",
        "        mask = cv2.imread(all_pairs[i][1])[:, :, 0]\n",
        "    ax3.flat[k].imshow(img, cmap='gray')\n",
        "    ax3.flat[k].contour((mask > 127).astype(float), levels=[0.5], colors=['red'])\n",
        "    ax3.flat[k].set_title(Path(all_pairs[i][0]).name[:20])\n",
        "    ax3.flat[k].axis('off')\n",
        "plt.suptitle('Sample images + mask contours'); plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'eda_samples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda-findings",
      "metadata": {},
      "source": [
        "#### Raw Discovery EDA — Findings & Next Steps\n",
        "\n",
        "Our Raw Discovery EDA focused on understanding the dataset's characteristics before model development. We found:\n",
        "\n",
        "- **No apparent label leakage:** Visual inspection of sample images showed no patient IDs or other text that could bias the model, suggesting a clean dataset.\n",
        "\n",
        "- **Generally dark images & consistent shapes:** Mean pixel intensities ranged from ~13 to ~71, indicating that the images are predominantly dark. All sampled images consistently maintained a shape of **(749, 709)**.\n",
        "\n",
        "- **Severe class imbalance:** A critical finding is the extreme class imbalance — in the sampled set, 100% of images were categorized as **high-risk** (IMT ≥ 0.9 mm), with 0% **normal**. This is clearly visible in the IMT distribution from the EDA outputs.\n",
        "\n",
        "**Next steps** derived from this EDA:\n",
        "\n",
        "1. **Address severe class imbalance** (e.g. oversampling, class weights, or collecting more diverse data) so the model does not collapse to predicting only high-risk.\n",
        "2. **Use image preprocessing** such as contrast enhancement (e.g. **CLAHE**, already applied in Section 2) to improve visibility in these generally dark images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7277102b",
      "metadata": {},
      "source": [
        "### 3c. Data loading (applies Section 2 preprocessing)\n",
        "\n",
        "**Use this section for loading data.** It uses the **same** preprocessing as Section 2 (the `cleaner`): CLAHE + DWT + resize + normalize to $[0,1]$. There is only one preprocessing pipeline; Section 2 defines it, this cell applies it.\n",
        "\n",
        "Pipeline: load image/mask → apply `cleaner` (CLAHE + DWT from Section 2) → resize to `IMG_SIZE` → output in $[0,1]$ (image and mask transforms mirrored)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569bf749",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Section 2 preprocessing (cleaner) + resize + [0,1]. Run Section 2 first to define 'cleaner'.\n",
        "IMG_SIZE = (224, 224)  # use (512, 512) for full Swin if GPU memory allows\n",
        "\n",
        "def load_and_prepare(img_path, mask_path, cleaner, size=IMG_SIZE):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        img = cv2.imread(img_path)[:, :, 0]\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        mask = cv2.imread(mask_path)[:, :, 0]\n",
        "    img = img.astype(np.float32) / (np.max(img) + 1e-8)\n",
        "    img = cleaner(img, apply_clahe=True, apply_dwt=True)\n",
        "    img = cv2.resize(img, (size[1], size[0]), interpolation=cv2.INTER_LINEAR)\n",
        "    mask = cv2.resize(mask, (size[1], size[0]), interpolation=cv2.INTER_NEAREST)\n",
        "    mask = (mask > 127).astype(np.float32)\n",
        "    return img[None], mask[None]  # (1,H,W), (1,H,W)\n",
        "\n",
        "def load_split(pairs, cleaner, size=IMG_SIZE):\n",
        "    X, y = [], []\n",
        "    for img_path, mask_path in pairs:\n",
        "        xi, yi = load_and_prepare(img_path, mask_path, cleaner, size)\n",
        "        X.append(xi)\n",
        "        y.append(yi)\n",
        "    return np.stack(X, axis=0).astype(np.float32), np.stack(y, axis=0).astype(np.float32)\n",
        "\n",
        "X_train, y_train = load_split(train_pairs, cleaner)\n",
        "X_val, y_val = load_split(val_pairs, cleaner)\n",
        "X_test, y_test = load_split(test_pairs, cleaner)\n",
        "spacing_mm = 0.04  # mm/pixel (typical US; use metadata if available)\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}, spacing_mm_per_pixel: {spacing_mm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "finetuning-intro",
      "metadata": {},
      "source": [
        "## 4. Fine-tuning\n",
        "\n",
        "Now that the data and masks are **technically perfect** (cleaned, preprocessed, and split), we proceed to **fine-tuning**: initialize the model (or load a pretrained checkpoint) and train it on the carotid dataset. The steps below define the architecture, optionally load pretrained weights, then run the training and validation loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81faf4c1",
      "metadata": {},
      "source": [
        "### 4a. Model initialization (Swin-UNETR)\n",
        "\n",
        "Architecture from MONAI. Key hyperparameters: **feature_size** (e.g. 48), **patch_size** (usually 2 for Swin), **num_heads** (e.g. 12). Optionally load pretrained weights for fine-tuning (e.g. from a medical imaging checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ba328d",
      "metadata": {},
      "outputs": [],
      "source": [
        "img_size = IMG_SIZE\n",
        "model = SwinUNETR(\n",
        "    img_size=img_size,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    spatial_dims=2,\n",
        "    use_checkpoint=True,\n",
        "    feature_size=48,\n",
        "    num_heads=(3, 6, 12, 24),\n",
        "    patch_size=2,\n",
        "    window_size=7,\n",
        ").to(device)\n",
        "pretrained_path = MODEL_DIR / \"model_swinvit.pt\"\n",
        "if pretrained_path.exists():\n",
        "    state = torch.load(pretrained_path, map_location=device)\n",
        "    model.load_state_dict(state.get(\"model\", state), strict=False)\n",
        "    print(\"Loaded pretrained weights for fine-tuning.\")\n",
        "print(f\"Swin-UNETR 2D model, params: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df2ab9a",
      "metadata": {},
      "source": [
        "### 4b. Training & validation loop (fine-tuning)\n",
        "\n",
        "**Addressing the 100% high-risk imbalance:**\n",
        "- **Weighted loss:** DiceCELoss (Dice + Cross-Entropy) focuses the model on the narrow carotid wall rather than the vast background.\n",
        "- **Augmentation:** Elastic deformations (e.g. Rand2DElasticd) simulate probe pressure and increase effective data variety.\n",
        "\n",
        "**Loop:** 70/15/15 split; **validation metric** = Mean Dice Score; **early stopping** when validation loss stops improving to avoid overfitting.\n",
        "\n",
        "**If the channel diagnostic showed correct order but Dice ~0.04:** Pred and GT both have ~2.5% foreground — the model predicts the right *proportion* but in the *wrong places* (spatial mismatch). The loss below uses **Dice + weighted Cross-Entropy** (foreground weight = 15) so the model is pushed to put probability on the wall pixels, not just anywhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f56881",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Foreground-weighted loss: GT has ~2% foreground; weight CE so model focuses on wall pixels (not just proportion).\n",
        "FOREGROUND_WEIGHT = 15.0  # increase (e.g. 20) if Dice still low\n",
        "dice_loss_fn = DiceLoss(include_background=False, softmax=True, to_onehot_y=True)\n",
        "ce_loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, FOREGROUND_WEIGHT], dtype=torch.float32, device=device))\n",
        "def criterion(out, target):\n",
        "    return dice_loss_fn(out, target) + ce_loss_fn(out, target)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "max_epochs = 50\n",
        "total_steps = max(1, len(train_pairs) // 4) * max_epochs\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train.squeeze(1).astype(np.int64)))\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "train_loss_history, val_dice_history = [], []\n",
        "best_val_dice, best_epoch, patience_counter = 0.0, 0, 0\n",
        "best_state = None\n",
        "early_stop_patience = 10\n",
        "\n",
        "for ep in range(max_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch_x)\n",
        "        loss = criterion(out, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    train_loss_history.append(mean_loss)\n",
        "\n",
        "    model.eval()\n",
        "    dice_metric.reset()\n",
        "    with torch.no_grad():\n",
        "        pred = model(torch.from_numpy(X_val).to(device))\n",
        "        pred_soft = torch.softmax(pred, dim=1)\n",
        "        # MONAI DiceMetric(include_background=False): Channel 0 = Background, Channel 1 = Foreground (IMT).\n",
        "        # Labels: 0=bg, 1=fg. one_hot -> (N,H,W,2); permute -> (N,2,H,W). Must match pred_soft (N,2,H,W).\n",
        "        y_val_labels = torch.from_numpy(y_val.squeeze(1).astype(np.int64)).to(device)\n",
        "        y_onehot = torch.nn.functional.one_hot(y_val_labels, num_classes=2).permute(0, 3, 1, 2).float()\n",
        "        # If your Dice is stuck ~0.04, try swapping channels (model may output fg in channel 0):\n",
        "        # pred_soft = pred_soft[:, [1, 0], :, :]\n",
        "        if ep == 0:\n",
        "            print(f\"  [debug] pred_soft.shape={tuple(pred_soft.shape)}, y_onehot.shape={tuple(y_onehot.shape)}\")\n",
        "            print(f\"  [debug] pred foreground (ch1) mean={pred_soft[:, 1].mean().item():.4f}, GT foreground mean={y_onehot[:, 1].mean().item():.4f}\")\n",
        "        dice_metric(y_pred=pred_soft, y=y_onehot)\n",
        "    val_dice = dice_metric.aggregate().item()\n",
        "    val_dice_history.append(val_dice)\n",
        "\n",
        "    if val_dice > best_val_dice:\n",
        "        best_val_dice = val_dice\n",
        "        best_epoch = ep + 1\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    print(f\"Epoch {ep+1}/{max_epochs}  loss: {mean_loss:.4f}  val Dice: {val_dice:.4f}\")\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"Early stopping at epoch {ep+1} (no improvement for {early_stop_patience} epochs). Best val Dice: {best_val_dice:.4f} at epoch {best_epoch}\")\n",
        "        break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "    model.to(device)\n",
        "print(\"Training done.\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_loss_history) + 1), train_loss_history, 'b-', label='Train loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(val_dice_history) + 1), val_dice_history, 'g-', label='Val Dice')\n",
        "plt.axhline(best_val_dice, color='gray', linestyle='--', label=f'Best {best_val_dice:.3f}')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Dice'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.suptitle('StrokeLink – training & validation')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training-validation-analysis",
      "metadata": {},
      "source": [
        "#### Quick channel diagnostic (run after at least one training epoch)\n",
        "\n",
        "If you see **val Dice stuck at 0.0454**, run the cell below. It prints prediction vs GT channel means and **Dice with and without channel swap**. If \"Dice (swapped)\" is much higher than \"Dice (current)\", the model outputs foreground in channel 0 — uncomment the swap in the training loop and re-train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "channel-diagnostic",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run once after training to check channel order (no need to re-train yet)\n",
        "from monai.metrics import DiceMetric\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(torch.from_numpy(X_val).to(device))\n",
        "    pred_soft = torch.softmax(pred, dim=1)\n",
        "    y_val_labels = torch.from_numpy(y_val.squeeze(1).astype(np.int64)).to(device)\n",
        "    y_onehot = torch.nn.functional.one_hot(y_val_labels, num_classes=2).permute(0, 3, 1, 2).float()\n",
        "print(f\"pred_soft.shape = {tuple(pred_soft.shape)},  y_onehot.shape = {tuple(y_onehot.shape)}\")\n",
        "print(f\"Pred channel 0 (bg) mean = {pred_soft[:, 0].mean().item():.4f},  channel 1 (fg) mean = {pred_soft[:, 1].mean().item():.4f}\")\n",
        "print(f\"GT   channel 0 (bg) mean = {y_onehot[:, 0].mean().item():.4f},  channel 1 (fg) mean = {y_onehot[:, 1].mean().item():.4f}\")\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric(y_pred=pred_soft, y=y_onehot)\n",
        "dice_current = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "dice_metric(y_pred=pred_soft[:, [1, 0], :, :], y=y_onehot)  # swap channels\n",
        "dice_swapped = dice_metric.aggregate().item()\n",
        "print(f\"Dice (current) = {dice_current:.4f}   |   Dice (swapped ch0<->ch1) = {dice_swapped:.4f}\")\n",
        "if dice_swapped > dice_current * 1.5:\n",
        "    print(\">> Swap gives much higher Dice → Uncomment 'pred_soft = pred_soft[:, [1, 0], :, :]' in the training loop and re-run training.\")\n",
        "else:\n",
        "    print(\">> Swap did not help → Channel order is likely correct; low Dice is from imbalance/small structure or learning.\")\n",
        "\n",
        "# Interpretation: Pred and GT both ~2.5% foreground → model predicts right proportion but in WRONG places (spatial mismatch).\n",
        "# Fix: Use a foreground-weighted loss (see training cell) so the model is pushed to put probability on the wall, not just anywhere."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training-validation-analysis",
      "metadata": {},
      "source": [
        "#### Analysis of training and validation results\n",
        "\n",
        "**Observations:** Training loss typically decreases (e.g. from ~0.06 in Epoch 1 to ~0.02 by Epoch 11), while **validation Dice can remain very low** (e.g. ~0.05) across epochs. Early stopping then halts training after no improvement for several epochs. This indicates the model is fitting the training set but **not generalizing** to the validation set.\n",
        "\n",
        "**Potential causes:**\n",
        "\n",
        "- **\"Ghost background\" / channel mismatch (very common):** A constant Dice ~0.0454 often means a **metric configuration error**, not total failure. MONAI's `DiceMetric(include_background=False)` assumes **Channel 0 = Background, Channel 1 = Foreground**. If the model's outputs are flipped, or one-hot labels don't match (same shape, channel 1 = foreground), the metric can compare the wrong channels and yield a near-constant low value. **Fix:** Check the first-epoch debug prints (`pred_soft.shape`, `y_onehot.shape`, pred foreground mean). If pred foreground mean is ~0, try swapping channels: `pred_soft = pred_soft[:, [1, 0], :, :]` before calling the metric.\n",
        "- **Severe class imbalance:** If validation masks have very few foreground pixels, Dice is unstable and a small misalignment gives ~0.05. Inspect the validation prediction plots (cell below) to see if the model outputs a \"blob\" that is slightly off the wall.\n",
        "- **Model / loss:** Consider loss weighting, learning rate, or optimizer if channels are correct and the model still doesn't generalize.\n",
        "\n",
        "**Next steps:** (1) Run the cell below to inspect validation data and **visualize predictions** (input | GT mask | pred foreground prob). (2) If pred foreground mean is ~0 in the training debug print, uncomment the channel swap in the training loop and re-run. (3) Tune loss or hyperparameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inspect-val-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect validation data: foreground fraction and sample images (run after training)\n",
        "val_masks = y_val.squeeze(1)  # (N, H, W)\n",
        "foreground_frac = (val_masks > 0).astype(np.float32).reshape(val_masks.shape[0], -1).mean(axis=1)\n",
        "print(f\"Validation: {val_masks.shape[0]} samples. Foreground (wall) fraction per sample: min={foreground_frac.min():.4f}, max={foreground_frac.max():.4f}, mean={foreground_frac.mean():.4f}\")\n",
        "if foreground_frac.mean() < 0.01:\n",
        "    print(\">> Very few foreground pixels in validation — low Dice is expected; consider class weights or inspecting mask encoding.\")\n",
        "n_show = min(4, len(X_val))\n",
        "fig, ax = plt.subplots(2, n_show, figsize=(2 * n_show, 4))\n",
        "for i in range(n_show):\n",
        "    ax[0, i].imshow(X_val[i].squeeze(), cmap='gray')\n",
        "    ax[0, i].set_title(f\"Val {i+1} fg={foreground_frac[i]:.3f}\")\n",
        "    ax[0, i].axis('off')\n",
        "    ax[1, i].imshow(y_val[i].squeeze(), cmap='viridis')\n",
        "    ax[1, i].set_title('Mask')\n",
        "    ax[1, i].axis('off')\n",
        "plt.suptitle('Validation samples: image (top) and mask (bottom)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'validation_inspection.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Visualize model predictions vs GT (detect channel mismatch or \"blob slightly off\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(torch.from_numpy(X_val[:n_show]).to(device))\n",
        "    pred_soft = torch.softmax(pred, dim=1)\n",
        "    pred_fg = pred_soft[:, 1].cpu().numpy()  # foreground channel\n",
        "    pred_class = pred_soft.argmax(dim=1).cpu().numpy()\n",
        "fig2, ax2 = plt.subplots(3, n_show, figsize=(2 * n_show, 6))\n",
        "for i in range(n_show):\n",
        "    ax2[0, i].imshow(X_val[i].squeeze(), cmap='gray')\n",
        "    ax2[0, i].set_title('Input')\n",
        "    ax2[0, i].axis('off')\n",
        "    ax2[1, i].imshow(y_val[i].squeeze(), cmap='viridis')\n",
        "    ax2[1, i].set_title('GT mask')\n",
        "    ax2[1, i].axis('off')\n",
        "    ax2[2, i].imshow(pred_fg[i], cmap='hot', vmin=0, vmax=1)\n",
        "    ax2[2, i].set_title(f'Pred fg prob (ch1) mean={pred_fg[i].mean():.3f}')\n",
        "    ax2[2, i].axis('off')\n",
        "plt.suptitle('Validation: input | GT mask | predicted foreground probability (channel 1)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'validation_predictions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e3e443",
      "metadata": {},
      "source": [
        "## 5. Evaluation on test set: Dice + IMT MAE (mm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9db855",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "with torch.no_grad():\n",
        "    pred = model(torch.from_numpy(X_test).to(device))\n",
        "    pred_soft = torch.softmax(pred, dim=1)\n",
        "    # Channel 0=bg, 1=fg. Match MONAI DiceMetric(include_background=False). If Dice stuck ~0.04, try: pred_soft = pred_soft[:, [1, 0], :, :]\n",
        "    y_test_labels = torch.from_numpy(y_test.squeeze(1).astype(np.int64)).to(device)\n",
        "    y_onehot = torch.nn.functional.one_hot(y_test_labels, num_classes=2).permute(0, 3, 1, 2).float()\n",
        "    dice_metric(y_pred=pred_soft, y=y_onehot)\n",
        "    pred_class = pred_soft.argmax(dim=1).cpu().numpy()\n",
        "test_dice = dice_metric.aggregate().item()\n",
        "imt_mae = imt_mae_mm(pred_class, y_test.squeeze(1).astype(np.int32), spacing_mm, lumen_label=2, wall_label=1)\n",
        "imt_str = f\"{imt_mae:.4f}\" if np.isfinite(imt_mae) else \"N/A\"\n",
        "print(f\"Test Dice: {test_dice:.4f}  |  IMT MAE (mm): {imt_str}\")\n",
        "print(f\"StrokeLink triage: IMT ≥ {IMT_HIGH_RISK_MM} mm = high risk (refer to Gasabo District)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6efe0bf6",
      "metadata": {},
      "source": [
        "## 6. Save model for the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b0ebf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = MODEL_DIR / \"carotid_swin_unetr_2d.pt\"\n",
        "torch.save({\n",
        "    \"model\": model.state_dict(), \"img_size\": img_size, \"in_channels\": 1, \"out_channels\": 2,\n",
        "    \"imt_high_risk_mm\": IMT_HIGH_RISK_MM, \"spacing_mm_per_pixel\": spacing_mm\n",
        "}, model_path)\n",
        "print(f\"Model saved to {model_path} (for FastAPI + Flutter app)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
